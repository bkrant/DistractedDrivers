{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json, shutil\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# created a file called 'utils.py' to store convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named utils",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-93b468cdfdce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named utils"
     ]
    }
   ],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to your training images\n",
    "train_dir = '/home/ubuntu/nbs/data/statefarm/train'\n",
    "# point to the 'driver_imgs_list.csv'\n",
    "lookup = '/home/ubuntu/nbs/data/statefarm/driver_imgs_list.csv'\n",
    "# point to the validation directory, which will be created in the next block\n",
    "val_dir = '/home/ubuntu/nbs/data/statefarm/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/nbs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a directory, called 'validation', and into it, creates 10 subdirs, one for each class.\n",
    "directory = 'valid'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "cwd = os.getcwd()\n",
    "path = val_dir\n",
    "[os.mkdir('{}/c{}'.format(path, i)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the 'driver_imgs_list.csv' file\n",
    "df = pd.read_csv(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 26 unique drivers in the train set. \n",
    "# We want 20%, so pick 5 at random\n",
    "driver_names = ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075', 'p081']\n",
    "driver_names_for_validation = random.sample(driver_names,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver_names_for_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-12214d2383cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# filter df by selecting rows with the subjects we want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_names_for_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'driver_names_for_validation' is not defined"
     ]
    }
   ],
   "source": [
    "# filter df by selecting rows with the subjects we want\n",
    "df = df[df['subject'].isin(driver_names_for_validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the move here:\n",
    "i = 0\n",
    "for index, row in df.iterrows():\n",
    "    to_move = train_dir + '/' +  row['classname'] + '/' + row['img']\n",
    "    #print to_move\n",
    "    i = i + 1\n",
    "    move_to = val_dir + '/' +  row['classname']\n",
    "    #print move_to\n",
    "    shutil.move(to_move, move_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22424, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files to move:  4285\n",
      "files moved:    4285\n"
     ]
    }
   ],
   "source": [
    "print('files to move: ' ,df.shape[0])\n",
    "print('files moved:   ' ,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data/statefarm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "#Set path to sample/ path if desired\n",
    "path = DATA_HOME_DIR + '/' #'/sample/'\n",
    "test_path = DATA_HOME_DIR + '/test/' #We use all the test data\n",
    "results_path=DATA_HOME_DIR + '/results/'\n",
    "train_path=path + '/train/'\n",
    "valid_path=path + '/valid/'\n",
    "model_path = path + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  path = \"data/statefarm/\"\n",
    "  results_path = \"data/statefarm/results/\"  \n",
    "# path = \"data/dogscatsredux/sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a pretrained VGG model with **Vgg16** class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is simply to use a model that has been fully created for us, which can recognise a wide variety (1,000 categories) of images. We will use 'VGG', which won the 2014 Imagenet competition, and is a very simple model to create and understand. The VGG Imagenet team created both a larger, slower, slightly more accurate model (*VGG  19*) and a smaller, faster model (*VGG 16*). We will be using VGG 16 since the much slower performance of VGG19 is generally not worth the very minor improvement in accuracy.\n",
    "\n",
    "We have created a python class, *Vgg16*, which makes using the VGG 16 model very straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our class, and instantiate\n",
    "#import vgg16; reload(vgg16)\n",
    "#from vgg16 import Vgg16\n",
    "from vgg16bn import Vgg16BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16BN()\n",
    "model=vgg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18139 images belonging to 10 classes.\n",
      "Found 4285 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# batches shuffle must be set to False when pre-computing features\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size, shuffle=False)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = vgg.get_batches(path+'test', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18139 images belonging to 10 classes.\n",
      "Found 4285 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_val_feat = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)\n",
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    model = Sequential( [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(1024, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(1024, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_bn_layers(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/5\n",
      "18139/18139 [==============================] - 15s - loss: 3.8609 - acc: 0.4367 - val_loss: 1.0032 - val_acc: 0.7260\n",
      "Epoch 2/5\n",
      "18139/18139 [==============================] - 15s - loss: 0.5034 - acc: 0.8678 - val_loss: 1.1222 - val_acc: 0.7587\n",
      "Epoch 3/5\n",
      "18139/18139 [==============================] - 15s - loss: 0.2158 - acc: 0.9385 - val_loss: 0.8781 - val_acc: 0.7827\n",
      "Epoch 4/5\n",
      "18139/18139 [==============================] - 15s - loss: 0.1244 - acc: 0.9640 - val_loss: 0.8472 - val_acc: 0.7998\n",
      "Epoch 5/5\n",
      "18139/18139 [==============================] - 15s - loss: 0.0986 - acc: 0.9708 - val_loss: 0.9192 - val_acc: 0.7708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f39f54dd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=5, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/2\n",
      "18139/18139 [==============================] - 15s - loss: 0.0834 - acc: 0.9738 - val_loss: 0.8113 - val_acc: 0.8033\n",
      "Epoch 2/2\n",
      "18139/18139 [==============================] - 15s - loss: 0.0710 - acc: 0.9772 - val_loss: 0.9061 - val_acc: 0.7916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f3bf9b090>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0651 - acc: 0.9814 - val_loss: 1.0106 - val_acc: 0.7564\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0597 - acc: 0.9816 - val_loss: 1.0413 - val_acc: 0.7732\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0557 - acc: 0.9834 - val_loss: 0.8192 - val_acc: 0.8021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f39f54ed0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0581 - acc: 0.9824 - val_loss: 1.0302 - val_acc: 0.7769\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0523 - acc: 0.9847 - val_loss: 1.0706 - val_acc: 0.7673\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0435 - acc: 0.9875 - val_loss: 1.1860 - val_acc: 0.7692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f3bfdae10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    model = get_bn_layers(p)\n",
    "    model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))\n",
    "    model.optimizer.lr=0.1\n",
    "    model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))\n",
    "    model.optimizer.lr=0.01\n",
    "    model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, val_labels))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/4\n",
      "18139/18139 [==============================] - 15s - loss: 3.8196 - acc: 0.4442 - val_loss: 0.8214 - val_acc: 0.7683\n",
      "Epoch 2/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.5007 - acc: 0.8696 - val_loss: 0.7264 - val_acc: 0.7965\n",
      "Epoch 3/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.2139 - acc: 0.9411 - val_loss: 0.8832 - val_acc: 0.7652\n",
      "Epoch 4/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.1291 - acc: 0.9619 - val_loss: 0.9563 - val_acc: 0.7634\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/1\n",
      "18139/18139 [==============================] - 15s - loss: 0.0970 - acc: 0.9716 - val_loss: 0.9099 - val_acc: 0.7809\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0696 - acc: 0.9782 - val_loss: 0.7867 - val_acc: 0.8070\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0643 - acc: 0.9810 - val_loss: 1.1012 - val_acc: 0.7848\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0649 - acc: 0.9815 - val_loss: 0.8806 - val_acc: 0.7853\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/4\n",
      "18139/18139 [==============================] - 15s - loss: 3.7635 - acc: 0.4450 - val_loss: 0.8800 - val_acc: 0.7643\n",
      "Epoch 2/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.5185 - acc: 0.8663 - val_loss: 0.9473 - val_acc: 0.7624\n",
      "Epoch 3/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.2221 - acc: 0.9380 - val_loss: 0.9711 - val_acc: 0.7685\n",
      "Epoch 4/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.1236 - acc: 0.9633 - val_loss: 0.7871 - val_acc: 0.7935\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/1\n",
      "18139/18139 [==============================] - 15s - loss: 0.0871 - acc: 0.9728 - val_loss: 1.0292 - val_acc: 0.7580\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0730 - acc: 0.9763 - val_loss: 1.1335 - val_acc: 0.7596\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0799 - acc: 0.9756 - val_loss: 1.0622 - val_acc: 0.7792\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0675 - acc: 0.9787 - val_loss: 1.1523 - val_acc: 0.7531\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/4\n",
      "18139/18139 [==============================] - 15s - loss: 3.7969 - acc: 0.4413 - val_loss: 0.8115 - val_acc: 0.7718\n",
      "Epoch 2/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.5039 - acc: 0.8690 - val_loss: 0.8201 - val_acc: 0.7683\n",
      "Epoch 3/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.2153 - acc: 0.9375 - val_loss: 0.7408 - val_acc: 0.7930\n",
      "Epoch 4/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.1312 - acc: 0.9612 - val_loss: 0.8084 - val_acc: 0.7846\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/1\n",
      "18139/18139 [==============================] - 15s - loss: 0.0972 - acc: 0.9703 - val_loss: 0.8691 - val_acc: 0.7881\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0777 - acc: 0.9759 - val_loss: 1.0317 - val_acc: 0.7622\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0695 - acc: 0.9796 - val_loss: 0.8655 - val_acc: 0.7883\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0579 - acc: 0.9809 - val_loss: 1.1068 - val_acc: 0.7631\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/4\n",
      "18139/18139 [==============================] - 15s - loss: 3.6859 - acc: 0.4533 - val_loss: 0.7413 - val_acc: 0.7725\n",
      "Epoch 2/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.4887 - acc: 0.8728 - val_loss: 0.9042 - val_acc: 0.7694\n",
      "Epoch 3/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.2038 - acc: 0.9433 - val_loss: 0.9847 - val_acc: 0.7617\n",
      "Epoch 4/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.1287 - acc: 0.9611 - val_loss: 1.0217 - val_acc: 0.7561\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/1\n",
      "18139/18139 [==============================] - 15s - loss: 0.0963 - acc: 0.9713 - val_loss: 0.8967 - val_acc: 0.7797\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0770 - acc: 0.9766 - val_loss: 0.8610 - val_acc: 0.7874\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0702 - acc: 0.9783 - val_loss: 0.9791 - val_acc: 0.7972\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0604 - acc: 0.9813 - val_loss: 1.0755 - val_acc: 0.7855\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/4\n",
      "18139/18139 [==============================] - 15s - loss: 3.7978 - acc: 0.4431 - val_loss: 0.8146 - val_acc: 0.7566\n",
      "Epoch 2/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.4921 - acc: 0.8719 - val_loss: 0.9663 - val_acc: 0.7482\n",
      "Epoch 3/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.2142 - acc: 0.9405 - val_loss: 1.0188 - val_acc: 0.7328\n",
      "Epoch 4/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.1309 - acc: 0.9629 - val_loss: 0.9300 - val_acc: 0.7664\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/1\n",
      "18139/18139 [==============================] - 15s - loss: 0.0905 - acc: 0.9727 - val_loss: 0.9655 - val_acc: 0.7459\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0870 - acc: 0.9734 - val_loss: 0.9409 - val_acc: 0.7594\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0631 - acc: 0.9795 - val_loss: 0.8778 - val_acc: 0.7876\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0624 - acc: 0.9811 - val_loss: 1.0137 - val_acc: 0.7680\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/4\n",
      "18139/18139 [==============================] - 15s - loss: 3.8341 - acc: 0.4382 - val_loss: 0.8993 - val_acc: 0.7389\n",
      "Epoch 2/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.5004 - acc: 0.8698 - val_loss: 1.0138 - val_acc: 0.7470\n",
      "Epoch 3/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.1996 - acc: 0.9422 - val_loss: 1.1207 - val_acc: 0.7389\n",
      "Epoch 4/4\n",
      "18139/18139 [==============================] - 15s - loss: 0.1306 - acc: 0.9608 - val_loss: 1.1503 - val_acc: 0.7424\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/1\n",
      "18139/18139 [==============================] - 15s - loss: 0.0911 - acc: 0.9722 - val_loss: 1.0416 - val_acc: 0.7655\n",
      "Train on 18139 samples, validate on 4285 samples\n",
      "Epoch 1/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0713 - acc: 0.9779 - val_loss: 0.9693 - val_acc: 0.7739\n",
      "Epoch 2/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0649 - acc: 0.9807 - val_loss: 1.1651 - val_acc: 0.7564\n",
      "Epoch 3/3\n",
      "18139/18139 [==============================] - 15s - loss: 0.0633 - acc: 0.9804 - val_loss: 0.9281 - val_acc: 0.7764\n"
     ]
    }
   ],
   "source": [
    "models = [fit_model() for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = path + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,m in enumerate(models):\n",
    "    m.save_weights(model_path+'fish-'+str(i)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,m in enumerate(models):\n",
    "    m.load_weights(model_path+'fish-'+str(i)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4285/4285 [==============================] - 1s     \n",
      "4285/4285 [==============================] - 1s     \n",
      "4285/4285 [==============================] - 1s     \n",
      "4285/4285 [==============================] - 1s     \n",
      "4285/4285 [==============================] - 1s     \n",
      "4285/4285 [==============================] - 1s     \n"
     ]
    }
   ],
   "source": [
    "evals = np.array([m.evaluate(conv_val_feat, val_labels, batch_size=batch_size) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8806,  0.7853],\n",
       "       [ 1.1523,  0.7531],\n",
       "       [ 1.1068,  0.7631],\n",
       "       [ 1.0755,  0.7855],\n",
       "       [ 1.0137,  0.768 ],\n",
       "       [ 0.9281,  0.7764]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0262,  0.7719])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(conv_test_feat, batch_size=batch_size) for m in models])\n",
    "save_array(model_path+'allpreds.dat', all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9dcd542fea52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'allpreds.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_preds' is not defined"
     ]
    }
   ],
   "source": [
    "load_array(model_path+'allpreds.dat', all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_preds = all_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79726, 10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 79726, 10)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4285,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = test_batches.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.metrics.categorical_accuracy(, avg_preds).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79726"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79726, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = do_clip(avg_preds,0.7719)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0264,  0.0264,  0.0264,  0.0264,  0.0264,  0.0264,  0.0264,  0.2709,  0.0264,  0.5177],\n",
       "       [ 0.39  ,  0.0259,  0.0259,  0.0259,  0.0259,  0.0259,  0.0259,  0.0259,  0.0259,  0.403 ],\n",
       "       [ 0.0299,  0.0299,  0.0299,  0.0299,  0.7084,  0.0299,  0.0299,  0.0299,  0.0523,  0.0299],\n",
       "       [ 0.0287,  0.0287,  0.0287,  0.0287,  0.0287,  0.0287,  0.0903,  0.0287,  0.68  ,  0.0287],\n",
       "       [ 0.028 ,  0.0902,  0.028 ,  0.028 ,  0.028 ,  0.028 ,  0.028 ,  0.0347,  0.3544,  0.3529]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = np.array([f[8:] for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['img_81601.jpg', 'img_14887.jpg', 'img_62885.jpg', 'img_45125.jpg', 'img_22633.jpg'], \n",
       "      dtype='|S14')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submm=np.stack([ids,subm[:,0],subm[:,1],subm[:,2],subm[:,3],subm[:,4],subm[:,5],subm[:,6],subm[:,7],subm[:,8],subm[:,9]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['img_81601.jpg', '0.0264233928174', '0.0264233928174', '0.0264233928174', '0.0264233928174',\n",
       "        '0.0264233928174', '0.0264233928174', '0.0264233928174', '0.270909547806',\n",
       "        '0.0264233928174', '0.517703354359'],\n",
       "       ['img_14887.jpg', '0.389978826046', '0.0258737504482', '0.0258737504482', '0.0258737504482',\n",
       "        '0.0258737504482', '0.0258737504482', '0.0258737504482', '0.0258737504482',\n",
       "        '0.0258737504482', '0.403031110764'],\n",
       "       ['img_62885.jpg', '0.0299060307443', '0.0299060307443', '0.0299060307443', '0.0299060307443',\n",
       "        '0.708422899246', '0.0299060307443', '0.0299060307443', '0.0299060307443',\n",
       "        '0.0523288920522', '0.0299060307443']], \n",
       "      dtype='|S32')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submm[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(submm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.insert(0, 'img', [a[4:] for a in test_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+\"submission_sf_ens.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79726"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>own/img_81601.jpg</td>\n",
       "      <td>img_81601.jpg</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.270909547806</td>\n",
       "      <td>0.0264233928174</td>\n",
       "      <td>0.517703354359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>own/img_14887.jpg</td>\n",
       "      <td>img_14887.jpg</td>\n",
       "      <td>0.389978826046</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.0258737504482</td>\n",
       "      <td>0.403031110764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>own/img_62885.jpg</td>\n",
       "      <td>img_62885.jpg</td>\n",
       "      <td>0.0299060307443</td>\n",
       "      <td>0.0299060307443</td>\n",
       "      <td>0.0299060307443</td>\n",
       "      <td>0.0299060307443</td>\n",
       "      <td>0.708422899246</td>\n",
       "      <td>0.0299060307443</td>\n",
       "      <td>0.0299060307443</td>\n",
       "      <td>0.0299060307443</td>\n",
       "      <td>0.0523288920522</td>\n",
       "      <td>0.0299060307443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>own/img_45125.jpg</td>\n",
       "      <td>img_45125.jpg</td>\n",
       "      <td>0.028706651181</td>\n",
       "      <td>0.028706651181</td>\n",
       "      <td>0.028706651181</td>\n",
       "      <td>0.028706651181</td>\n",
       "      <td>0.028706651181</td>\n",
       "      <td>0.028706651181</td>\n",
       "      <td>0.0903350710869</td>\n",
       "      <td>0.028706651181</td>\n",
       "      <td>0.680011630058</td>\n",
       "      <td>0.028706651181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>own/img_22633.jpg</td>\n",
       "      <td>img_22633.jpg</td>\n",
       "      <td>0.0279557984322</td>\n",
       "      <td>0.0902168154716</td>\n",
       "      <td>0.0279557984322</td>\n",
       "      <td>0.0279557984322</td>\n",
       "      <td>0.0279557984322</td>\n",
       "      <td>0.0279557984322</td>\n",
       "      <td>0.0279557984322</td>\n",
       "      <td>0.0347320996225</td>\n",
       "      <td>0.354382961988</td>\n",
       "      <td>0.352933317423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 img              0                1                2  \\\n",
       "0  own/img_81601.jpg  img_81601.jpg  0.0264233928174  0.0264233928174   \n",
       "1  own/img_14887.jpg  img_14887.jpg   0.389978826046  0.0258737504482   \n",
       "2  own/img_62885.jpg  img_62885.jpg  0.0299060307443  0.0299060307443   \n",
       "3  own/img_45125.jpg  img_45125.jpg   0.028706651181   0.028706651181   \n",
       "4  own/img_22633.jpg  img_22633.jpg  0.0279557984322  0.0902168154716   \n",
       "\n",
       "                 3                4                5                6  \\\n",
       "0  0.0264233928174  0.0264233928174  0.0264233928174  0.0264233928174   \n",
       "1  0.0258737504482  0.0258737504482  0.0258737504482  0.0258737504482   \n",
       "2  0.0299060307443  0.0299060307443   0.708422899246  0.0299060307443   \n",
       "3   0.028706651181   0.028706651181   0.028706651181   0.028706651181   \n",
       "4  0.0279557984322  0.0279557984322  0.0279557984322  0.0279557984322   \n",
       "\n",
       "                 7                8                9               10  \n",
       "0  0.0264233928174   0.270909547806  0.0264233928174   0.517703354359  \n",
       "1  0.0258737504482  0.0258737504482  0.0258737504482   0.403031110764  \n",
       "2  0.0299060307443  0.0299060307443  0.0523288920522  0.0299060307443  \n",
       "3  0.0903350710869   0.028706651181   0.680011630058   0.028706651181  \n",
       "4  0.0279557984322  0.0347320996225   0.354382961988   0.352933317423  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/statefarm/submission_sf_ens.csv' target='_blank'>data/statefarm/submission_sf_ens.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/nbs/data/statefarm/submission_sf_ens.csv"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(path+'submission_sf_ens.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
